{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrency 1 and Databases 4\n",
    "\n",
    "## Threads, the GIL, transactions and functional programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads\n",
    "\n",
    "Some code examples here taken from the Python Cookbook, freely available at http://chimera.labs.oreilly.com/books/1230000000393/index.html. This is an excellent book and will help you with your python programming and with your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fib(n):\n",
    "    return fib(n - 1) + fib(n - 2) if n > 1 else n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still running\n",
      "Still running\n",
      "Still running\n",
      "Still running\n",
      "Still running\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "t = Thread(target=fib, args=(35,)) \n",
    "t.start()\n",
    "while t.is_alive(): \n",
    "    print('Still running')\n",
    "    sleep(1)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But running something computationally intensive in a thread is a bad idea,\n",
    "because python has a global interpreter lock (GIL). Thus, Python allows one thread to run in the interpreter at any given time. \n",
    "\n",
    "This makes Python threads better for handling concurrent execution where there is a lot of waiting...waiting for stuff from a file, from the network, from a database, sleeping, etc\n",
    "\n",
    "#### What is concurrency, exactly?\n",
    "\n",
    " A concurrent program has multiple logical threads, which may or may not run in parallel.\n",
    " \n",
    ">Rob Pike puts it: Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.\n",
    "\n",
    "Concurrency is when your program needs to handle multiple simultaneous **events**: multiple network connections to a database, a repl connection, some other process, etc.\n",
    " \n",
    "\n",
    "- concurrent programs are often non-deterministic —the order will depend on the precise timing of events. So its best to hadle independent streams of execution, or communicate between these streams in loosely coupled ways such as queues.\n",
    "- concurrency is critical to responsivity. Your web broser handles many resource pull ins together, some in separate threads. But even within the main process it might need to handle a mouse event and an image draw/network fetch at the same time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O bound code vs cpu bound code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "from time import time\n",
    "\n",
    "\n",
    "def sleepy(context): #like io\n",
    "    i=0\n",
    "    while i < 10:\n",
    "        print(\"context {}, {} -- {} Sleepy!\".format(context, i, int(time())), flush=True)\n",
    "        #you could replace this by a request to a website in a crawler\n",
    "        sleep(1)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def cpuy(context):\n",
    "    for i in range(35):\n",
    "        val = fib(i)\n",
    "        print(\"{} cpuy fib({}) is {}\".format(context, i, val))\n",
    "\n",
    "        \n",
    "def main_cpu():\n",
    "    start = time()\n",
    "    cpuy(\"serial 1\")\n",
    "    cpuy(\"serial 2\")\n",
    "    print(\"serial cpu elapsed:\", time() - start)\n",
    "    start=time()\n",
    "    #t = Thread(target=sleepy)\n",
    "    #t.start()\n",
    "    t2 = Thread(target=cpuy, args=(\"threaded 2\",))\n",
    "    t2.start()\n",
    "    # Main thread will read and process input\n",
    "    cpuy(\"threaded 1\")\n",
    "    print(\"thread cpu elapsed:\", time() - start)\n",
    "\n",
    "def main_sleep():\n",
    "    # Second thread will print the hello message. Starting as a daemon means\n",
    "    # the thread will not prevent the process from exiting.\n",
    "    start = time()\n",
    "    sleepy(\"serial 1\")\n",
    "    sleepy(\"serial 2\")\n",
    "    print(\"serial sleep elapsed:\", time() - start)\n",
    "    start=time()\n",
    "    t = Thread(target=sleepy, args=(\"threaded 2\",))\n",
    "    t.start()\n",
    "    # Main thread will read and process input\n",
    "    sleepy(\"threaded 1\")\n",
    "    print(\"thread sleep elapsed:\", time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serial 1 cpuy fib(0) is 0\n",
      "serial 1 cpuy fib(1) is 1\n",
      "serial 1 cpuy fib(2) is 1\n",
      "serial 1 cpuy fib(3) is 2\n",
      "serial 1 cpuy fib(4) is 3\n",
      "serial 1 cpuy fib(5) is 5\n",
      "serial 1 cpuy fib(6) is 8\n",
      "serial 1 cpuy fib(7) is 13\n",
      "serial 1 cpuy fib(8) is 21\n",
      "serial 1 cpuy fib(9) is 34\n",
      "serial 1 cpuy fib(10) is 55\n",
      "serial 1 cpuy fib(11) is 89\n",
      "serial 1 cpuy fib(12) is 144\n",
      "serial 1 cpuy fib(13) is 233\n",
      "serial 1 cpuy fib(14) is 377\n",
      "serial 1 cpuy fib(15) is 610\n",
      "serial 1 cpuy fib(16) is 987\n",
      "serial 1 cpuy fib(17) is 1597\n",
      "serial 1 cpuy fib(18) is 2584\n",
      "serial 1 cpuy fib(19) is 4181\n",
      "serial 1 cpuy fib(20) is 6765\n",
      "serial 1 cpuy fib(21) is 10946\n",
      "serial 1 cpuy fib(22) is 17711\n",
      "serial 1 cpuy fib(23) is 28657\n",
      "serial 1 cpuy fib(24) is 46368\n",
      "serial 1 cpuy fib(25) is 75025\n",
      "serial 1 cpuy fib(26) is 121393\n",
      "serial 1 cpuy fib(27) is 196418\n",
      "serial 1 cpuy fib(28) is 317811\n",
      "serial 1 cpuy fib(29) is 514229\n",
      "serial 1 cpuy fib(30) is 832040\n",
      "serial 1 cpuy fib(31) is 1346269\n",
      "serial 1 cpuy fib(32) is 2178309\n",
      "serial 1 cpuy fib(33) is 3524578\n",
      "serial 1 cpuy fib(34) is 5702887\n",
      "serial 2 cpuy fib(0) is 0\n",
      "serial 2 cpuy fib(1) is 1\n",
      "serial 2 cpuy fib(2) is 1\n",
      "serial 2 cpuy fib(3) is 2\n",
      "serial 2 cpuy fib(4) is 3\n",
      "serial 2 cpuy fib(5) is 5\n",
      "serial 2 cpuy fib(6) is 8\n",
      "serial 2 cpuy fib(7) is 13\n",
      "serial 2 cpuy fib(8) is 21\n",
      "serial 2 cpuy fib(9) is 34\n",
      "serial 2 cpuy fib(10) is 55\n",
      "serial 2 cpuy fib(11) is 89\n",
      "serial 2 cpuy fib(12) is 144\n",
      "serial 2 cpuy fib(13) is 233\n",
      "serial 2 cpuy fib(14) is 377\n",
      "serial 2 cpuy fib(15) is 610\n",
      "serial 2 cpuy fib(16) is 987\n",
      "serial 2 cpuy fib(17) is 1597\n",
      "serial 2 cpuy fib(18) is 2584\n",
      "serial 2 cpuy fib(19) is 4181\n",
      "serial 2 cpuy fib(20) is 6765\n",
      "serial 2 cpuy fib(21) is 10946\n",
      "serial 2 cpuy fib(22) is 17711\n",
      "serial 2 cpuy fib(23) is 28657\n",
      "serial 2 cpuy fib(24) is 46368\n",
      "serial 2 cpuy fib(25) is 75025\n",
      "serial 2 cpuy fib(26) is 121393\n",
      "serial 2 cpuy fib(27) is 196418\n",
      "serial 2 cpuy fib(28) is 317811\n",
      "serial 2 cpuy fib(29) is 514229\n",
      "serial 2 cpuy fib(30) is 832040\n",
      "serial 2 cpuy fib(31) is 1346269\n",
      "serial 2 cpuy fib(32) is 2178309\n",
      "serial 2 cpuy fib(33) is 3524578\n",
      "serial 2 cpuy fib(34) is 5702887\n",
      "serial cpu elapsed: 14.029454946517944\n",
      "threaded 2 cpuy fib(0) is 0\n",
      "threaded 2 cpuy fib(1) is 1\n",
      "threaded 2 cpuy fib(2) is 1\n",
      "threaded 2 cpuy fib(3) is 2\n",
      "threaded 2 cpuy fib(4) is 3\n",
      "threaded 2 cpuy fib(5) is 5\n",
      "threaded 2 cpuy fib(6) is 8\n",
      "threaded 2 cpuy fib(7) is 13\n",
      "threaded 2 cpuy fib(8) is 21\n",
      "threaded 2 cpuy fib(9) is 34\n",
      "threaded 2 cpuy fib(10) is 55\n",
      "threaded 2 cpuy fib(11) is 89\n",
      "threaded 2 cpuy fib(12) is 144\n",
      "threaded 2 cpuy fib(13) is 233\n",
      "threaded 2 cpuy fib(14) is 377\n",
      "threaded 2 cpuy fib(15) is 610\n",
      "threaded 2 cpuy fib(16) is 987\n",
      "threaded 2 cpuy fib(17) is 1597\n",
      "threaded 2 cpuy fib(18) is 2584\n",
      "threaded 2 cpuy fib(19) is 4181\n",
      "threaded 1 cpuy fib(0) is 0\n",
      "threaded 1 cpuy fib(1) is 1\n",
      "threaded 1 cpuy fib(2) is 1\n",
      "threaded 1 cpuy fib(3) is 2\n",
      "threaded 1 cpuy fib(4) is 3\n",
      "threaded 1 cpuy fib(5) is 5\n",
      "threaded 1 cpuy fib(6) is 8\n",
      "threaded 1 cpuy fib(7) is 13\n",
      "threaded 1 cpuy fib(8) is 21\n",
      "threaded 1 cpuy fib(9) is 34\n",
      "threaded 1 cpuy fib(10) is 55\n",
      "threaded 1 cpuy fib(11) is 89\n",
      "threaded 1 cpuy fib(12) is 144\n",
      "threaded 1 cpuy fib(13) is 233\n",
      "threaded 1 cpuy fib(14) is 377\n",
      "threaded 1 cpuy fib(15) is 610\n",
      "threaded 1 cpuy fib(16) is 987\n",
      "threaded 1 cpuy fib(17) is 1597\n",
      "threaded 1 cpuy fib(18) is 2584\n",
      "threaded 1 cpuy fib(19) is 4181\n",
      "threaded 2 cpuy fib(20) is 6765\n",
      "threaded 1 cpuy fib(20) is 6765\n",
      "threaded 2 cpuy fib(21) is 10946\n",
      "threaded 1 cpuy fib(21) is 10946\n",
      "threaded 2 cpuy fib(22) is 17711\n",
      "threaded 1 cpuy fib(22) is 17711\n",
      "threaded 2 cpuy fib(23) is 28657\n",
      "threaded 1 cpuy fib(23) is 28657\n",
      "threaded 2 cpuy fib(24) is 46368\n",
      "threaded 1 cpuy fib(24) is 46368\n",
      "threaded 1 cpuy fib(25) is 75025\n",
      "threaded 2 cpuy fib(25) is 75025\n",
      "threaded 1 cpuy fib(26) is 121393\n",
      "threaded 2 cpuy fib(26) is 121393\n",
      "threaded 1 cpuy fib(27) is 196418\n",
      "threaded 2 cpuy fib(27) is 196418\n",
      "threaded 1 cpuy fib(28) is 317811\n",
      "threaded 2 cpuy fib(28) is 317811\n",
      "threaded 1 cpuy fib(29) is 514229\n",
      "threaded 2 cpuy fib(29) is 514229\n",
      "threaded 1 cpuy fib(30) is 832040\n",
      "threaded 2 cpuy fib(30) is 832040\n",
      "threaded 1 cpuy fib(31) is 1346269\n",
      "threaded 2 cpuy fib(31) is 1346269\n",
      "threaded 1 cpuy fib(32) is 2178309\n",
      "threaded 2 cpuy fib(32) is 2178309\n",
      "threaded 1 cpuy fib(33) is 3524578\n",
      "threaded 2 cpuy fib(33) is 3524578\n",
      "threaded 1 cpuy fib(34) is 5702887\n",
      "thread cpu elapsed: 18.94524312019348\n",
      "threaded 2 cpuy fib(34) is 5702887\n"
     ]
    }
   ],
   "source": [
    "main_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context serial 1, 0 -- 1478101306 Sleepy!\n",
      "context serial 1, 1 -- 1478101307 Sleepy!\n",
      "context serial 1, 2 -- 1478101308 Sleepy!\n",
      "context serial 1, 3 -- 1478101309 Sleepy!\n",
      "context serial 1, 4 -- 1478101310 Sleepy!\n",
      "context serial 1, 5 -- 1478101311 Sleepy!\n",
      "context serial 1, 6 -- 1478101312 Sleepy!\n",
      "context serial 1, 7 -- 1478101313 Sleepy!\n",
      "context serial 1, 8 -- 1478101314 Sleepy!\n",
      "context serial 1, 9 -- 1478101315 Sleepy!\n",
      "context serial 2, 0 -- 1478101316 Sleepy!\n",
      "context serial 2, 1 -- 1478101317 Sleepy!\n",
      "context serial 2, 2 -- 1478101318 Sleepy!\n",
      "context serial 2, 3 -- 1478101319 Sleepy!\n",
      "context serial 2, 4 -- 1478101320 Sleepy!\n",
      "context serial 2, 5 -- 1478101321 Sleepy!\n",
      "context serial 2, 6 -- 1478101322 Sleepy!\n",
      "context serial 2, 7 -- 1478101323 Sleepy!\n",
      "context serial 2, 8 -- 1478101324 Sleepy!\n",
      "context serial 2, 9 -- 1478101325 Sleepy!\n",
      "serial sleep elapsed: 20.031989812850952\n",
      "context threaded 2, 0 -- 1478101326 Sleepy!\n",
      "context threaded 1, 0 -- 1478101326 Sleepy!\n",
      "context threaded 1, 1 -- 1478101327 Sleepy!\n",
      "context threaded 2, 1 -- 1478101327 Sleepy!\n",
      "context threaded 1, 2 -- 1478101328 Sleepy!context threaded 2, 2 -- 1478101328 Sleepy!\n",
      "\n",
      "context threaded 2, 3 -- 1478101329 Sleepy!\n",
      "context threaded 1, 3 -- 1478101329 Sleepy!\n",
      "context threaded 1, 4 -- 1478101330 Sleepy!\n",
      "context threaded 2, 4 -- 1478101330 Sleepy!\n",
      "context threaded 2, 5 -- 1478101331 Sleepy!\n",
      "context threaded 1, 5 -- 1478101331 Sleepy!\n",
      "context threaded 2, 6 -- 1478101332 Sleepy!\n",
      "context threaded 1, 6 -- 1478101332 Sleepy!\n",
      "context threaded 1, 7 -- 1478101333 Sleepy!\n",
      "context threaded 2, 7 -- 1478101333 Sleepy!\n",
      "context threaded 1, 8 -- 1478101334 Sleepy!\n",
      "context threaded 2, 8 -- 1478101334 Sleepy!\n",
      "context threaded 2, 9 -- 1478101335 Sleepy!\n",
      "context threaded 1, 9 -- 1478101335 Sleepy!\n",
      "thread sleep elapsed: 10.018816947937012\n"
     ]
    }
   ],
   "source": [
    "main_sleep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to write threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n 0\n",
      "n 1\n",
      "n 2\n",
      "n 3\n",
      "n 4\n",
      "n 5\n"
     ]
    }
   ],
   "source": [
    "#do this\n",
    "# Code to execute in an independent thread\n",
    "import time\n",
    "def counter(nmax):\n",
    "    n=0\n",
    "    while n <= nmax:\n",
    "        print('n', n) \n",
    "        n += 1 \n",
    "        sleep(1)\n",
    "# Create and launch a thread\n",
    "from threading import Thread\n",
    "t = Thread(target=counter, args=(5,)) \n",
    "t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n 0\n",
      "n 1\n",
      "n 2\n",
      "n 3\n",
      "n 4\n",
      "n 5\n"
     ]
    }
   ],
   "source": [
    "#dont do this\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "class Counter(Thread): \n",
    "    def __init__(self, nmax): \n",
    "        super().__init__()\n",
    "        self.nmax = nmax \n",
    "        self.n=0\n",
    "    def run(self):\n",
    "        while self.n <= self.nmax:\n",
    "            print('n', self.n) \n",
    "            self.n += 1 \n",
    "            sleep(1)\n",
    "c = Counter(5)\n",
    "c.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n 0\n",
      "n 1\n",
      "n 2\n",
      "n 3\n",
      "n 4\n",
      "n 5\n"
     ]
    }
   ],
   "source": [
    "#because you might want to run it in a new process!\n",
    "import multiprocessing\n",
    "p = multiprocessing.Process(target=counter, args=(5,)) \n",
    "p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communicating with threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put data 0.06013972309645832 5\n",
      "got data 0.06013972309645832 1\n",
      "put data 0.9194857108251694 4\n",
      "got data 0.9194857108251694 2\n",
      "put data 0.8370650677184625 3\n",
      "got data 0.8370650677184625 3\n",
      "put data 0.21314236649834428 2\n",
      "got data 0.21314236649834428 4\n",
      "put data 0.6169153817399435 1\n",
      "got data 0.6169153817399435 5\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue \n",
    "import random\n",
    "from threading import Thread\n",
    "def producer(out_q):\n",
    "    counter=5\n",
    "    while counter > 0:\n",
    "        # Produce some data\n",
    "        data = random.random()\n",
    "        print(\"put data\", data, counter)\n",
    "        out_q.put(data)\n",
    "        time.sleep(2)\n",
    "        counter -=1\n",
    "        \n",
    "# A thread that consumes data\n",
    "def consumer(in_q): \n",
    "    counter = 0\n",
    "    while True:\n",
    "        # Get some data\n",
    "        try:\n",
    "            data = in_q.get(timeout=5) # Process the data ...\n",
    "            counter += 1\n",
    "            print(\"got data\", data, counter)\n",
    "        except:\n",
    "            break\n",
    "        # Create the shared queue and launch both threads\n",
    "q = Queue()\n",
    "t1 = Thread(target=consumer, args=(q,))\n",
    "t2 = Thread(target=producer, args=(q,))\n",
    "t1.start()\n",
    "t2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process models: especially in databases\n",
    "\n",
    "I saw the characterization of database systems along the lines we have talked about in this course first in http://db.cs.berkeley.edu/papers/fntdb07-architecture.pdf . This is an incredible review paper, if a little bit dated. But I stongly recommend you read it now: you will get a lot out of it. Between this paper, \"Designing Database Intensive Applications\" from Oreilly, and what you have learnt, you will be able to read all modern database papers.\n",
    "\n",
    "\n",
    "We need concurrency because you ou are writing a multi-user database: it must take more than one request at a time.\n",
    "\n",
    "You must plan how to handle these requests. Lets talk in terms of a unit of execution, something used to handle one user, or do one transaction. Then the question arises: what do these map to?\n",
    "\n",
    "- an operating system process (multiprocessing). Scheduled by OS kernel, state maintained in private, per-process address space and execution context. Really slow switching.\n",
    "- an operating system thread (multithreading). Scheduled by OS, but does not have its own address space and context; shares it with other threads within the same process. Fast switching\n",
    "- an inside-process thread (light weight thread or coroutine) with asynchronous IO: one process, so the adress and context are shared with the other \"user\" threads or **coroutines**. Any long running IO can block the process, so must be done asynchronously and perhaps in another OS thread. Super fast switching.\n",
    "\n",
    "We'll call the unit-of-execution in the case of a DBMS, the DBMS Worker. In general, this will map onto 1 client request, or one transaction\n",
    "\n",
    "#### Process per DBMS worker model\n",
    "\n",
    "This really is two models: spawn a process every thime a new request comes in, or use a process pool from which a process is chosen to handle the request. Once the request is done, the process goes back to the pool.\n",
    "This is easy to implement, and many databases were first started when OS based thread support was poor. Thus they used this model.\n",
    "\n",
    "The complication here is sharing memory structures, especially for the lock table and buffer pool(this is a memory pool into which database pages are mapped to use: these must be flushed to disk on transactions but also serve as a cache for gets). This uses POSIX shared memory or System V ipc (see http://semanchuk.com/philip/sysv_ipc/)\n",
    "\n",
    "Also note that memory-mapping can be used for this purpose. Memory maps map part of a file to memory, and let you address the filea s if you were addressing a memory buffer. They take care of bring in parts of the file to memory under the hood. This implementation is used in lmdb and boltdb to provide fast transactions.\n",
    "\n",
    "(Note that OS's also provide buffer caches in which they load existing files in memory. But this is not shareable, and many databases prefer to manage this mapping themselves)\n",
    "\n",
    "Because of the overhead of context-switching, process per dbms worker models tend to be not very scaleable. But in Python, with the GIL, it might be a preferred model if there is any substantial CPU bound processing like a stored procedure. Still, one could run a Threadpool, with the ability to use a process as a coprocessor. See http://chimera.labs.oreilly.com/books/1230000000393/ch12.html#_dealing_with_the_gil_and_how_to_stop_worrying_about_it .\n",
    "\n",
    "The buffer pool is stored in shared memory thats shared by all these processes. Its important to use shared memory. You might be tempted to create the database process in the memory of the main process, and after fork write to it in the children. This is a bad idea as memory semantics for children are copy on write, so that you will be then writing to the new processes memory space and thus database replica, and not to the main database.\n",
    "\n",
    "Process per worker model is supported by IBM DB2, PostgreSQL, and Oracle.\n",
    "\n",
    "#### Worker per thread or threadpool model\n",
    "\n",
    "Here the main thread listens for the database connections, and each connection is then allocated a new thread. This is a model thats really simple to start of with, but has issues with deadlocks and stuff: you must be careful. Although, these challenges are also present in the multiprocess model due to use of shared memory and thus the need to lock there.\n",
    "\n",
    "The thread per worker model scales well to many concurrent connections (as long as you dont have the GIL, and even then, if the computation part is small: although note that if you had an overall faster speed by using an in memory db and thus minimizing IO, most of your remaining overhead would be in the GIL unless you used C code/Cython to access the memory and do some computations on it.) \n",
    "\n",
    "IBM DB2 has a worker per thread model, as do MS SQL server, MySQL, Informix, and Sybase.\n",
    "\n",
    "In this case, the buffer pool is simply a heap resident data structure. where as in the process based model, it is allocated in shared memory so that it is available to all processes.\n",
    "\n",
    "When a thread or process needs a page to be read from disk it will generate an io request (read-into) with the disk address and the memory address. Doing stuff in fixed size pages (where the size is oprimized for cache and the size of your data) makes this process fast.\n",
    "\n",
    "TODO:??: The reverse is dont to write (write from). The lock table uses a similar implementation.\n",
    "\n",
    "#### Light weight threads or coroutines\n",
    "\n",
    "In the past, when OS thread support was not so good, and now again since the resurgence of interest in asynchronous programming, many widely used databases implement their own light-weight threads. To quote the stonebraker paper:\n",
    "\n",
    "These lightweight threads, or DBMS threads, replace the role of the OS threads described in the previous section. Each DBMS thread is programmed to manage its own state, to perform all potentially block- ing operations (e.g., I/Os) via non-blocking, asynchronous interfaces, and to frequently yield control to a scheduling routine that dispatches among these tasks.\n",
    "\n",
    "Sybase and Informix are examples of this mode of usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the potential for deadlock, programs that use locks should be written in a way such that each thread is only allowed to acquire one lock at a time.\n",
    "\n",
    "Here is a locked dictionary, a proxy for a very simple database if you like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "class LockableDict: \n",
    "    def __init__(self):\n",
    "        self._d={}\n",
    "        self._dlock={}\n",
    "        \n",
    "    def __getitem__(self, attr):\n",
    "        return self._d[attr]\n",
    "    \n",
    "    def __setitem__(self, attr, val):\n",
    "        if attr not in self._d:\n",
    "            self._dlock[attr]=threading.Lock()\n",
    "        with self._dlock[attr]:\n",
    "            self._d[attr] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = LockableDict()\n",
    "l['a'] = 3\n",
    "l['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement a database server to front this next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race conditions\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/counterupdate.png)\n",
    "\n",
    "Our code is still problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "class SharedCounter: \n",
    "    def __init__(self, initial_value = 0):\n",
    "        self._value = initial_value\n",
    "        \n",
    "    def incr(self, context, delta=1):\n",
    "        temp = self._value\n",
    "        temp = temp + delta\n",
    "        if temp %2 ==0:\n",
    "            sleep(0.0001)\n",
    "        self._value = temp\n",
    "        print(\"{} {}\".format(context, self._value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_counter(n, c, context):\n",
    "    for i in range(n):\n",
    "        c.incr(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 1\n",
      "main 2\n",
      "main 3\n",
      "thread 2\n",
      "thread 3\n",
      "main 4\n",
      "main 5\n",
      "thread 4\n",
      "thread 5\n",
      "main 6\n",
      "main 7\n",
      "thread 6\n",
      "thread 7\n",
      "main 8\n",
      "main 9\n",
      "thread 8\n",
      "thread 9\n",
      "main 10\n",
      "main 11\n",
      "thread 10\n",
      "thread 11\n",
      "main 12\n",
      "main 13\n",
      "thread 12\n",
      "thread 13\n",
      "main 14\n",
      "main 15\n",
      "thread 14\n",
      "thread 15\n",
      "main 16\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "c = SharedCounter()\n",
    "t = Thread(target=run_counter, args=(15,c,\"thread\")) \n",
    "t.start()\n",
    "run_counter(15, c, \"main\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "class SharedCounterWithLock: \n",
    "    def __init__(self, initial_value = 0):\n",
    "        self._value = initial_value\n",
    "        self._value_lock = threading.Lock()\n",
    "        \n",
    "    def incr(self, context, delta=1):\n",
    "        with self._value_lock:\n",
    "            temp = self._value\n",
    "            temp = temp + delta\n",
    "#             if temp %2 ==0:\n",
    "#                 sleep(0.0001)\n",
    "            self._value = temp\n",
    "            print(\"{} {}\".format(context, self._value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 1\n",
      "thread 2\n",
      "thread 3\n",
      "thread 4\n",
      "thread 5\n",
      "thread 6\n",
      "thread 7\n",
      "thread 8\n",
      "thread 9\n",
      "thread 10\n",
      "thread 11\n",
      "thread 12\n",
      "thread 13\n",
      "thread 14\n",
      "thread 15\n",
      "main 16\n",
      "main 17\n",
      "main 18\n",
      "main 19\n",
      "main 20\n",
      "main 21\n",
      "main 22\n",
      "main 23\n",
      "main 24\n",
      "main 25\n",
      "main 26\n",
      "main 27\n",
      "main 28\n",
      "main 29\n",
      "main 30\n"
     ]
    }
   ],
   "source": [
    "cl = SharedCounterWithLock()\n",
    "t = Thread(target=run_counter, args=(15,cl,\"thread\")) \n",
    "t.start()\n",
    "run_counter(15, cl, \"main\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only single opcodes (in the virtual machine) ops are **thread-safe**, the notion that shared structures and code does not need to be locked. See https://en.wikipedia.org/wiki/Thread_safety . Ways to make things thread safe are atomicity (discussed soon), immutable data (also discusses soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrent access to databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions\n",
    "\n",
    "(most diagrams and all quotes are from Oreilly's Designing Data Intensive Applications: a book you should read if you, like me, enjoy this stuff).\n",
    "\n",
    "The general rules (we'll break these down soon...)\n",
    "- The batch of operations is viewed as a single atomic operation, so all of the operations either succeed together or fail together.\n",
    "- The database is in a valid state before and after the transaction.\n",
    "- The batch update appears to be isolated; other queries should never see a database state in which only some of the operations have been applied.\n",
    " \n",
    "Databases have a mechanism for wrapping a single or multiple processes into a **Transaction**. This means that the batch of operations either all happen (**commit**) or not happen at all (**abort**, **rollback**). This is called **atomicity**.\n",
    "\n",
    "In your career you will likely at some point have toevaluate which database you use for the job. Always ask: do I need \"ACID\" transactions: for OLTP you probably do, for OLAP the answer maybe a no.\n",
    "(The NOSQL movement made transactions into a casualty since they were distributed and worried about replication and partitioning. Be very careful evaluating transactional claims in NoSQL databases)\n",
    "\n",
    "The transactional guarantees are represented by the acronym `ACID`. The A is for atomicity, which we just described.\n",
    "\n",
    "* C is for **Consistency**: data invariants must be true. This is really a property of the application: eg accounting tables must be balanced. Databases can help with foreign keys, but this is a property of the app. We wont discuss this one further,\n",
    "* D is for **Durability**: once a transaction has comitted successfully, data comitted wont be forgotten. This requires persistent storage, or replication, or both\n",
    "* I is for **Isolation**. This is the most interesting of the lot, and critical to the sensible running of a databse. The idea is that transactions should not step on each other. Each transaction should pretends that its the only one running on the databse: in other words, as if the transactions wer completely serialized. In practice this would make things very slow, so we try different transactional guarantees that fall short of explicit serialization except in the situations that really need serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single vs multi-object\n",
    "\n",
    "Atomicity and Isolation are usually provided by most databases for single object writes, like a single set (or for that matter a single get) on one machine. As we shall see later, these are critical to prevent \"lost updates\" and are a big part of the transaction story, one that is implemented even in the new-fangled databases. \n",
    "\n",
    "But they are not the full story. We need multi-object transactions to\n",
    "\n",
    "- deal with foreign keys\n",
    "- deal with the denormalization seen in nosql's like mongo\n",
    "- deal with secondary indexes.\n",
    "\n",
    "Lets look at all of these scenarios using a relational setup. We'll revisit these in Friday's lab when we create an append-only binary-tree key-value database embedded in a single index (ie trivially clustered).\n",
    "\n",
    "We'll also revisit these concepts a few lectures later in the general context of programming, not just in databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is isolation important?\n",
    "\n",
    "It is hard to program without isolation. isolation is what guarantees stability. It is what makes sure that there are no dirty reads and dirty writes.\n",
    "\n",
    "From \"Designing data intensive applications\", here are the definitions:\n",
    "\n",
    "Dirty reads\n",
    "\n",
    ">One client reads another client’s writes before they have been committed. The **read committed isolation level** and stronger levels prevent dirty reads.\n",
    "\n",
    "\n",
    "Dirty writes\n",
    "\n",
    ">One client overwrites data that another client has written, but not yet committed. Almost all transaction implementations prevent dirty writes.\n",
    "\n",
    "Clearly, the notions of isolation are really the notions of concurrenvy: these issues will also occut when 2 programs access any data, in memory or in a database. In both cases locks and other ideas must be used to make sure that there is only one mutator at a time, and that an object is not exposed in an inconsistent state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Committed\n",
    "\n",
    "No dirty reads and no dirty writes.\n",
    "\n",
    "Dirty reads means that you can peek into the goings on inside a transaction and get at itermediate values. This could mean you see a value that would be later rolled back.\n",
    "\n",
    "With dirty writes you could be overwriting an uncomitted value. When we write we will use a lock to ensure no-one else can write.\n",
    "\n",
    "No dirty reads:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/nodirtyreads.png)\n",
    "\n",
    "\n",
    "Dirty Writes in multiple objects: \n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/dirtywrite.png)\n",
    "\n",
    "\n",
    "\n",
    "For dirty writes: \n",
    "In a regular Relational Database Management System (RDBMS) we use row-level locks. Another writer must wait until the lock is given up, and the reader will read the value before the lock was set as long as the write transaction is in progress.\n",
    "\n",
    "For dirty reads:\n",
    "We could use the same lock, and require the reader to briefly lock the object. But a write transaction may be a long one, there might be the need to do  multiple reads in the meanwhile.\n",
    "\n",
    "Thus, in Read Comitted mode, in essence we need to remember the pre-transaction value, ie the old comitted value and the also the new value being set by the transaction which holds the current write lock. \n",
    "\n",
    "**While this transaction runs, any other thread/transaction sees the old value**. Once this one commits, then the values read by the other transaction are switched. \n",
    "\n",
    "Read Comitted is almost always implemented: its the default in Oracle, Postgres, SQL Server, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Read Skew Problem and Snapshot Isolation\n",
    "\n",
    ">Non-repeatable reads:\n",
    ">A client sees different parts of the database at different points in time. This is most commonly prevented with snapshot isolation, which allows a transaction to read from a consistent snapshot at one point in time. It is usually implemented with multi-version concurrency control (MVCC).\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/readskew.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the diagram above, it makes perfect sence if Alice's two reads are separate read transactions. But what if they are one transaction and we are in read-comitted mode? Since the reads took a very long time and were on both sides of a kosher transaction, they might give Alice a heart attack. This is a non-repeatable read as if Alice were to re-issue the read transaction, acct 1 would now show 600. This problem is perfectly consistent with operations in read-comitted mode: the second read gets the switched value because the write transaction is completed.\n",
    "\n",
    "The fix for this would seem to be clear: if both the reads happen in one transaction, one must make sure that because this transaction happened before the write, it sees the older version of the accounts (500, 500). \n",
    "\n",
    "These thoughts form the basis of Multi-Version concurrency control, or MVCC, also known as snapshot isolation. This is needed for analytic queries testing integrity, backups, etc.\n",
    "\n",
    "The key principle is (from DDIA):\n",
    "> readers never block writers, and writers never block readers\n",
    "\n",
    "Here then are the rules:\n",
    "\n",
    ">1. At the start of each transaction, the database makes a list of all the other transations which are in progress (not yet committed or aborted) at that time. Any writes made by one of those transactions are ignored, even if the transaction subsequently commits.\n",
    "2. Any writes made by aborted transactions are ignored.\n",
    "3. Any writes made by transactions with a later transaction ID (i.e. which started after the current transaction started) are ignored, regardless of whether that transaction has committed.\n",
    "4. All other writes are visible to the application’s queries.\n",
    "\n",
    "Here's what snapshot isolation using MVCC looks like:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/mvcc.png)\n",
    "\n",
    "Snapshot isolation is supported by postgres, MySQL with the InnoDB storage engine, Oracle, SQL Server, and more, but may not be the default, as its less performant than read-committed isolation. \n",
    "\n",
    "Indexes in a MVCC database are hard to get right...and affect performance as they need updating. We'll see some ways around this in the lab.\n",
    "\n",
    "For bank accounts you need it, but for github you might not..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about writes?\n",
    "\n",
    "There is another problem we encounter here: \n",
    "\n",
    "### the lost update problem.\n",
    "\n",
    "Consider an upsert, where we read a value, do something with it, like the balance change above, and then write it. Consider two such upserting transactions T1 and T2. Because we lock writes, a transaction T1 might read a value V0, and then transaction T2 starting when T1 is on can only read value V0. Since T2 does not (yet) have lock, the code in a `get` uses the old value. This is good as the transaction T1 is still running.\n",
    "\n",
    "But here is the problem: T1 will create a new value, say with V0+1. It then relinquishes the lock. Now T2 comes gets lock, operates on V0, and makes it V0+2. \n",
    "\n",
    "Say V0 was 5 to start with. The notion of the process might have been 5->6->8. But we wont do that. Our latest value will be 7.\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/lostupdate.png)\n",
    "\n",
    "The fix to this is atomic updates/upserts. The `UPDATE` command in sql does just that, and if we build transactional facility into our database to do multiple gets, there is no reason not to extend it to an upsert. The entire upsert would grab the lock, and once that happens this would effectively serialize T1 and T2. Atomic updates are usually thus built by taking the lock when the object is read, or by forcing all atomic operations to be on one given thread.\n",
    "\n",
    "So thus it seems we can add a transaction manager, have an explicit notion of read, write, and upsert transactions, and have a reasonable snapshot isolated database.\n",
    "\n",
    "But even this does not solve some problems..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write skew\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/writeskew.png)\n",
    "\n",
    "Both Alice and Bob are last docs on a shift. They both want to leave. So they both go to their web portal and click on the \"check-out\" button. The app is using snalshot isolation. Since they both click at roughly the same time, before any one's check-out transaction has completed, the both \"get\" a count of two doctors on call and their transactions are allowed to proceed, leaving no docs. oops.\n",
    "\n",
    "This anomaly is called **write-skew** and happens as the transactions here are updating different rows.\n",
    "\n",
    "Another example is conflicting meetings being booked in a room at the same time.\n",
    "\n",
    "The pattern here is that there is a SELECT query that two transactions do that would have retuened different results based on which a write would have happened or not. But these two transactions were touching two different rows or parts of the dbase and were thus allowed under MVCC. You can think of write-skew as a generalization of lost-update.\n",
    "\n",
    "> This effect, where a write in one transaction changes the result of a search query in another transaction, is called a phantom. Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions like the examples we discussed, phantoms can lead to particularly tricky cases of write skew.\n",
    "\n",
    "One could solve these problems in multiple ways. The most general solution is to use serialized isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialized Isolation\n",
    "\n",
    "- one could actually run things in serial on one thread. With in memory databases this has become faster. Voltdb/h-store does this with replication, and stored procedures (in java and groovy, a JVM language).\n",
    "\n",
    "- stored procedures\n",
    "\n",
    "Stored procedures run inside the database address space, the idea being to eliminate the overhead of serialization and de-serialization, network transfer, etc (but a badly written stored procedure could be worse)\n",
    "\n",
    "- if multithreaded\n",
    "\n",
    "but if you want multithreaded, the old, but not gold since its slowness makes people not like it is 2-phase locking(2PL). \n",
    "\n",
    "- the ingredients are shared read locks and exclusive write locks.\n",
    "\n",
    "The process then is:\n",
    "\n",
    ">- if a transaction wants to read an object, it must first acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simultaneously, but if another transaction already has an exclusive lock on the object, the transaction must wait.\n",
    "- If a transaction wants to write to an object, it must first acquire the lock in exclu‐ sive mode. No other transaction may hold the lock at the same time (neither in shared nor in exclusive mode), so if there is any existing lock on the object, the transaction must wait.\n",
    "- If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. The upgrade works the same as getting an exclusive lock directly.\n",
    "- After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name “two- phase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional programming: IMMUTABLE DATA\n",
    "\n",
    "All of these are illustrations of simultaneous access to **mutable** data by concurrent threads. In a world of multi-core computers where lots of events must be handled, we should be thinking of ways to not have to deal with so much state.\n",
    "\n",
    "Functional programming has this notion of **referential transparency** where functions dont modify state: they are like maths functions: we can always substitute the snswer in place of the function because some hidden \"instance\" variable encapsulating state is not allowed to change a functions behaviour in functional programming.\n",
    "\n",
    "The key idea to get out of that is immutability of bindings and data structures. We can use this idea in writing databases to similify things and make implementing isolation levels easier with no-overwriting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutable BST\n",
    "\n",
    "We'll study the binary search tree as a proxy for the btree. Its not the best choice, as rebalancing and moving keys around creates havoc with seeking on disk, rather than being localizes to some pages, but its much more understandable.\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/immutablebst.png)\n",
    "\n",
    "(from purely functional data structures by Okasaki: great book BTW, with examples of balanced trees as well)\n",
    "\n",
    "- what is Append-Only\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/append1.png)\n",
    "\n",
    "- What is copy on write\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/cow1.png)\n",
    "![](https://dl.dropboxusercontent.com/u/75194/cow2.png)\n",
    "![](https://dl.dropboxusercontent.com/u/75194/cow3.png)\n",
    "\n",
    "See http://symas.com/mdb/20141120-BuildStuff-Lightning.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
