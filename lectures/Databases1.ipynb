{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://github.com/cs109/2015/raw/master/Lectures/Lecture4/ds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ">It took about three years before the BellKorâ€™s Pragmatic Chaos team managed to win the prize ... The winning algorithm was ... so complex that it was never implemented by Netflix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DATA ENGINEERING\n",
    "\n",
    "- compute: code, python, R, julia, spark, hadoop\n",
    "- storage/database: git, SQL, NoSQL, HBase, disk, memory\n",
    "- devops: AWS, docker, mesos, repeatability\n",
    "- product: database, web, API, viz, UI, story\n",
    "\n",
    "Different at different scales...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Pipelines these days\n",
    "\n",
    "If you are a datascientist or enginner in any small to mid sized company with a web presence, you might see something like this in your data infrastructure.\n",
    "\n",
    "This example is from @wrobstory: the SIMPLE pipelines:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/simplepipe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you are a data engineer, you'll be expected to architect the systems which take external data, and internal data, and combine them into databases which are both the source and target for analysis.\n",
    "\n",
    "The critical component of all this architecture is the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why learn databases\n",
    "\n",
    "- you shouldnt really implement one: very hard to get right\n",
    "- but you must understand how they work\n",
    "- data storage/munging are not just database concerns, Eg: Apache Parquet, array, dplyr, pandas\n",
    "- choosing a storage engine that is ok for your program\n",
    "- understanding query performance\n",
    "- transaction processing is not optimal for analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What kind of data access do you need?\n",
    "\n",
    "- relational: pandas, SQL: Postgres, sqlite, Hbase, VoltDB\n",
    "- document oriented: MongoDB, CouchDB\n",
    "- key-value: Riak, Memcached, leveldb\n",
    "- graph oriented: Neo4J\n",
    "- in-memory data-structure server: Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relational vs Document/key-value\n",
    "\n",
    "- today both are highly used: we have *polyglot persistence*\n",
    "- Mongo/Couch/etc are document oriented, store JSON documents\n",
    "- these have a higher locality of data: its like a really wide row with hierarchy\n",
    "- normalization vs denormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relational Model\n",
    "\n",
    "- a relation (table) is a collection of tuples. Each tuple is called a *row*\n",
    "- A collection of tables related to each other through common data values.\n",
    "- Everything in a column is values of one attributes\n",
    "- A cell is expected to be atomic\n",
    "- Tables are related to each other if they have columns called keys which represent the same values\n",
    "- SQL a declarative model: a query optimizer decides how to execute the query (if a field range covers 80% of values, should we use the index or the table?). Also parallelizable\n",
    "- *shredding* splits a document into multiple tables due to normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://github.com/cs109/2015/raw/master/Lectures/Lecture4/contributors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://github.com/cs109/2015/raw/master/Lectures/Lecture4/candidates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key-Value Model\n",
    "\n",
    "- like a dictionary\n",
    "- the database is the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Document Model\n",
    "\n",
    "- stores nested records\n",
    "- bad for many-to-many\n",
    "- storage locality good for access, bad for writing\n",
    "- couch, mongo, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalization vs denormalization\n",
    "\n",
    "- normalization is the use of pointers (id's from other tables) to point elsewhere (make tables)\n",
    "- denormalization is the following and expansion of the pointer\n",
    "- how would you represent this for the contributes and candidates above? (make JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Components to a database\n",
    "\n",
    "- client connection manager: what to do with incomings\n",
    "- transactional storage\n",
    "    - storage data structures and the log\n",
    "    - transactions and ACID: atomicity, consistency, isolation, durability\n",
    "- process model: coroutines, threads, processes\n",
    "- query model and language: query optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/dbmscomponents.png) (DBMS components from Hellerstein at al: Architecture of a Database System: circa 2007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://github.com/cs109/2015/raw/master/Lectures/Lecture4/sqlexecution.png)\n",
    "\n",
    "(from 7 databases in 7 weeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Indexing and Databases\n",
    "\n",
    "- an additional structure derived from the primary data\n",
    "- however a clustered index may actually store the data\n",
    "- this happens often in key-value databases, but can happen in tree based indexes as well\n",
    "- there is overhead on writes: indexes speed up queries but slow down writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary Search Trees\n",
    "\n",
    "We've seen sorting so far. \n",
    "\n",
    "Whever we want to maintain our search dataset in memory, sorted, we use something like a Binary Search Tree instead. \n",
    "\n",
    "They perform well with dynamic data where insertions and deletions are frequent, because of the so called $O(height)$ guarantees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Trees Supported operations\n",
    "\n",
    "Whenever we encounter a new data structure we should write down their ops and the performance of these ops. \n",
    "\n",
    "The idea for BSTs is to be like a sorted array but provide fast(logarithmic) inserts and deletes.\n",
    "\n",
    "- **Search** O(h)\n",
    "- **Select**(Order Statistic) O(h) : Needs Augmented Tree, up from O(1) on sorted arrays\n",
    "- **Minimum/Maximum** O(h), up from O(1) on sorted arrays\n",
    "- **Predecessor/Successor** O(h), up from O(1) on sorted arrays\n",
    "- **Rank** or Index O(h): Needs Augmented Tree\n",
    "- **In-Order Traversal** O(n)\n",
    "- **Insert** O(h)\n",
    "- **Delete** O(h)\n",
    "\n",
    "If we guarantee a balanced Tree:\n",
    "\n",
    "$$ h = lg(n)$$\n",
    "\n",
    "and thus most of these ops are $O(lg(n))$.\n",
    "\n",
    "And we dont have to pay the piper on the sort...\n",
    "\n",
    "![](bstproperty.png)\n",
    "\n",
    "![](oninserts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Btrees\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree1q.png)\n",
    "\n",
    "(from https://loveforprogramming.quora.com/Memory-locality-the-magic-of-B-Trees)\n",
    "\n",
    "- \"A linked sorted distributed range array with predefined sub array size which allows searches, sequential access, insertions, and deletions in logarithmic time. \"\n",
    "- it is a generalization of a binary tree\n",
    "- but the branching factor is much higher, and the depth thus smaller\n",
    "- btrees break database into pages, and read-or-write one page at a time. A page is about 4k in size (see https://www.tutorialspoint.com/operating_system/os_virtual_memory.htm )\n",
    "- leaf pages contain all the values and may represent a clustered index\n",
    "- the pointers in a btree are disk based pointers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/btree1.png)\n",
    "(from designing data intensive applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we update a key, a split can happen\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree2.png)\n",
    "\n",
    "(from designing data intensive applications)\n",
    "\n",
    "This is an in-place modification. The data structure is mutable. This can cause issues for transactions, and must be dealt with. \n",
    "\n",
    "Both splits and writing in-place are dangerous, so its normal for b-tree implementations to have a WAL, or write ahead log (such a log can also be used to manage transactions). Every operation on the btree is appended to this log file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/btree2q.png)\n",
    "\n",
    "In B+ trees, pointers amonst the leaf nodes make for an easier linear scan.\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/bplus.png)\n",
    "\n",
    "(from https://loveforprogramming.quora.com/Memory-locality-the-magic-of-B-Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Handling the different workloads\n",
    "\n",
    "- for smaller sizes any relational db will do\n",
    "- currently vendors focus on one or the other, not both\n",
    "- MS and SAP HANA support both but with different storage engines\n",
    "- OLTP need to be highly available, low latency\n",
    "- SQL (or any Pandasish syntax) is good for drilling down\n",
    "- warehousing: star schema with very wide fact table, but typically you focus on few columns at a time\n",
    "- btree indexes for oltp, bitmaps + btree for warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Storage components of a database\n",
    "\n",
    "- the heap file: this is where the rows or columns are stored\n",
    "- regular relational databases use row oriented heap files\n",
    "- an index file(s): this is where the index for a particular facet is stored\n",
    "- sometimes you have a clustered index ( all data stored in index) or covering index (some data is stored in index)\n",
    "- the WAL or write-ahead log: this is used to handle transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple start\n",
    "\n",
    "- start with index for key-value data\n",
    "- aka dictionary\n",
    "- in memory you are done. the index IS the database\n",
    "- hash tables are no good for range queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stillyoung'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database=dict()\n",
    "database['rahul']=\"aged\"\n",
    "database['pavlos']=\"ancient\"\n",
    "database['kobe']=\"stillyoung\"\n",
    "database['kobe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Doing it on disk\n",
    "\n",
    "- in the dict in memory, store a file offset instead\n",
    "- this \"HEAP\" file is an append only file, and thus also acts as a \"LOG\"\n",
    "- if you update, simply append a new entry and change the offset in the dict\n",
    "- this is an improvement to what you did for the linked-list based environment in the homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/hashmaplog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside into memoryview and bytes and bytearrays and struct\n",
    "\n",
    "\n",
    "#### Byteorder\n",
    "\n",
    "![](http://ptgmedia.pearsoncmg.com/images/chap3_0131411551/elementLinks/03fig09.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'little'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.byteorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'GIF89a\\xe8\\x03\\x90\\x01'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import struct\n",
    "fmt = '<3s3sHH'#little endian, 2 seq 3 bytes, 2 unsigned shorts\n",
    "with open(\"pcanim.gif\", 'rb') as fd:\n",
    "    readit = fd.read()\n",
    "    msg = memoryview(readit) #no copy\n",
    "header = msg[:10] # 10 byte view, no copy, imagine the savings\n",
    "bytes(header)# finally a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'GIF', b'89a', 1000, 400)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct.unpack(fmt, header)#type/version/width/height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readit[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bytes' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e0404bf63a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreadit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'bytes' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "readit[0]=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*bytearrays* as opposed to bytes, are read-write, which leads to being able to pre-allocate a \"buffer\", get a memoryview on it, and use the slice syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "with open(\"pcanim.gif\", 'rb') as fd:\n",
    "    data = bytearray(os.path.getsize(\"pcanim.gif\"))\n",
    "    fd.readinto(data)\n",
    "mv = memoryview(data)\n",
    "mv[0]=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a way to do something we couldn't achieve by any other means - read from a file (or receive from a socket) directly into the middle of some existing buffer\n",
    "\n",
    "```python\n",
    "buf = bytearray(...) # pre-allocated to the needed size\n",
    "mv = memoryview(buf)\n",
    "numread = f.readinto(mv[some_offset:])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "class Database():\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.byteorder=sys.byteorder\n",
    "        if not os.path.exists(file):\n",
    "            self.fd = open(file, \"xb+\", buffering=0) #see https://docs.python.org/3/library/functions.html#open\n",
    "            self.index={}\n",
    "        else:\n",
    "            self.fd = open(file, \"r+b\", buffering=0)\n",
    "            with open(file+\".idx\") as fdi:\n",
    "                items = [l.strip().split(':') for l in fdi.readlines()]\n",
    "                self.index = {k:int(v) for k,v in items}\n",
    "        self.readptr = self.fd.tell()\n",
    "        self.fd.seek(0,2)\n",
    "        self.writeptr = self.fd.tell()\n",
    "        \n",
    "        \n",
    "    def set(self, x, v):\n",
    "        if not isinstance(x, str):\n",
    "            raise ValueError(\"Key must be a string\")\n",
    "        bin_x = x.encode('utf-8')\n",
    "        sz_x=len(bin_x).to_bytes(1, byteorder=self.byteorder)\n",
    "        if not isinstance(v, str):\n",
    "            raise ValueError(\"Value must be a string\")\n",
    "        bin_v = v.encode('utf-8')\n",
    "        sz_v=len(bin_v).to_bytes(1, byteorder=self.byteorder)\n",
    "        try:\n",
    "            self.index[x]=self.writeptr\n",
    "            self.fd.seek(self.writeptr)\n",
    "            print(\"currently\", self.fd.tell())\n",
    "            self.fd.write(sz_x+sz_v+bin_x+bin_v)\n",
    "        except:\n",
    "            del self.index[x]\n",
    "        else:\n",
    "            self.writeptr=self.fd.tell()\n",
    "            \n",
    "    def get(self, x):\n",
    "        try:\n",
    "            offset = self.index[x]\n",
    "        except:\n",
    "            raise ValueError(\"{} is not in index\".format(x))\n",
    "        bin_x = x.encode('utf-8')\n",
    "        print(\"offset is\", offset)\n",
    "        self.readptr=offset\n",
    "        self.fd.seek(self.readptr)#seek to that point. we dont load anything in memory yet\n",
    "        sz_k = int.from_bytes(self.fd.read(1), byteorder=self.byteorder)\n",
    "        sz_v = int.from_bytes(self.fd.read(1), byteorder=self.byteorder)\n",
    "        self.fd.seek(sz_k,1)\n",
    "        readit=self.fd.read(sz_v).decode('utf-8')\n",
    "        print(\"now\", self.fd.tell())\n",
    "        return readit\n",
    "        \n",
    "    def close(self):\n",
    "        fdi=open(self.file+\".idx\",\"w\")\n",
    "        fdi.write(\"\\n\".join([k+\":\"+str(v) for k,v in self.index.items()]))\n",
    "        fdi.close()\n",
    "        self.fd.close()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm /tmp/test.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "db = Database(\"/tmp/test.db\")\n",
    "print(db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 0\n",
      "currently 11\n",
      "currently 23\n"
     ]
    }
   ],
   "source": [
    "db.set(\"rahul\", \"aged\")\n",
    "db.set(\"pavlos\", \"aged\")\n",
    "db.set(\"kobe\", \"stillyoung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0005\u0004rahulaged\u0006\u0004pavlosaged\u0004\r\n",
      "kobestillyoung"
     ]
    }
   ],
   "source": [
    "!cat /tmp/test.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rahul': 0, 'pavlos': 11, 'kobe': 23}\n",
      "offset is 11\n",
      "now 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(db.index)\n",
    "db.get(\"pavlos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 39\n"
     ]
    }
   ],
   "source": [
    "db.set(\"rahul\",\"young\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'young'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n",
      "young\n",
      "offset is 11\n",
      "now 23\n",
      "aged\n",
      "offset is 23\n",
      "now 39\n",
      "stillyoung\n"
     ]
    }
   ],
   "source": [
    "db=Database(\"/tmp/test.db\")\n",
    "print(db.get(\"rahul\"))\n",
    "print(db.get(\"pavlos\"))\n",
    "print(db.get(\"kobe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kobe': 23, 'pavlos': 51, 'rahul': 39}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.set(\"pavlos\", \"ancient\")\n",
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n",
      "young\n",
      "offset is 51\n",
      "now 66\n",
      "ancient\n",
      "offset is 23\n",
      "now 39\n",
      "stillyoung\n"
     ]
    }
   ],
   "source": [
    "print(db.get(\"rahul\"))\n",
    "print(db.get(\"pavlos\"))\n",
    "print(db.get(\"kobe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rahul:39\r\n",
      "pavlos:51\r\n",
      "kobe:23"
     ]
    }
   ],
   "source": [
    "!cat /tmp/test.db.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0005\u0004rahulaged\u0006\u0004pavlosaged\u0004\r\n",
      "kobestillyoung\u0005\u0005rahulyoung\u0006\u0007pavlosancient"
     ]
    }
   ],
   "source": [
    "!cat /tmp/test.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transaction Processing or Analytics?\n",
    "\n",
    "- Also known as OLTP vs OLAP/Warehousing\n",
    "- small query size vs aggregates over large ones\n",
    "- random writes from user input vs ordered ETL/stream\n",
    "- end user (amazon site) vs analyst (you)\n",
    "- GB to TB vs TB to PB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/ETL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Oriented Storage\n",
    "\n",
    "- heapfile or clustered index is a set of rows\n",
    "- we'll see details of this storage soon\n",
    "- index could be a tree with appropriate pointer to heapfile offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://www.simple-talk.com/iwritefor/articlefiles/1844-f4cc85b0-9ddb-44cc-93ef-a742fcc4f279.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Heap file and index: Column oriented storage\n",
    "\n",
    "- store values from each column together in separate storage\n",
    "- lends itself to compression with bitmap indexes and run-length encoding\n",
    "- this involves choosing an appropriate sort order\n",
    "- the index then can be the data (great for IN and AND queries): there is no pointers to \"elsewhere\"\n",
    "- compressed indexes can fit into cache and are usable by iterators\n",
    "- bitwise AND/OR can be done with vector processing\n",
    "- several different sort orders can be redundantly stored\n",
    "- writing is harder: updating a row touches many column files\n",
    "- but you can write an in-memory front sorted store (row or column), and eventually merge onto the disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://www.simple-talk.com/iwritefor/articlefiles/1844-4e2482bb-aaff-4ebd-8900-1946560479af.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data cubes\n",
    "\n",
    "- Basically a histogram of counts in bins for multiple fields\n",
    "- can give you fast marginals and conditionals in any combination of dimensions\n",
    "- expensive to update so only used for warehousing\n",
    "- such histograms are used by query optimizers as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### WAL for commits\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/wal1.png)\n",
    "![](https://dl.dropboxusercontent.com/u/75194/wal2.png)\n",
    "![](https://dl.dropboxusercontent.com/u/75194/wal3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Grammar of Data\n",
    "\n",
    "- provide simple verbs for simple things. These are functions corresponding to common data manipulation tasks\n",
    "( https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html )\n",
    "\n",
    "See https://gist.github.com/TomAugspurger/6e052140eaa5fdb6e8c0/ which has a comparison of r/dplyr and pandas. I stole and modified this table from there:\n",
    "\n",
    "``dplyr`` has a small set of nicely defined verbs. I've listed their closest pandas verbs.\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><b>VERB</b></th>\n",
    "    <th><b>dplyr</b></th>\n",
    "    <th><b>pandas</b></th>\n",
    "    <th><b>SQL</b></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>QUERY/SELECTION</td>\n",
    "    <td>filter() (and slice())</td>\n",
    "    <td>query() (and loc[], iloc[])</td>\n",
    "    <td>SELECT WHERE</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SORT</td>\n",
    "    <td>arrange()</td>\n",
    "    <td>sort()</td>\n",
    "    <td>ORDER BY</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SELECT-COLUMNS/PROJECTION</td>\n",
    "    <td>select() (and rename())</td>\n",
    "    <td>[](__getitem__) (and rename())</td>\n",
    "    <td>SELECT COLUMN</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SELECT-DISTINCT</td>\n",
    "    <td>distinct()</td>\n",
    "    <td>unique(),drop_duplicates()</td>\n",
    "    <td>SELECT DISTINCT COLUMN</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ASSIGN</td>\n",
    "    <td>mutate() (and transmute())</td>\n",
    "    <td>assign</td>\n",
    "    <td>ALTER/UPDATE</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>AGGREGATE</td>\n",
    "    <td>summarise()</td>\n",
    "    <td>describe(), mean(), max()</td>\n",
    "    <td>None, AVG(),MAX()</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SAMPLE</td>\n",
    "    <td>sample_n() and sample_frac()</td>\n",
    "    <td>sample()</td>\n",
    "    <td>implementation dep, use RAND()</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>GROUP-AGG</td>\n",
    "    <td>group_by/summarize</td>\n",
    "    <td>groupby/agg, count, mean</td>\n",
    "    <td>GROUP BY</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DELETE</td>\n",
    "    <td>?</td>\n",
    "    <td>drop/masking</td>\n",
    "    <td>DELETE/WHERE</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "We'll tackle joins in the sql tutorial, but until then, here is a rough understanding for them:\n",
    "\n",
    "https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll use sqlite here (and recommend Postgres for production purposes). Still sqlite is great for on-disk large databases which wont fit into memory. \n",
    "\n",
    "Its also built into Python, but to use the [command line tool](https://www.sqlite.org/cli.html), I recommend you install it: https://www.sqlite.org/download.html. I also recommend you download and install the sqlite browser: http://sqlitebrowser.org .\n",
    "\n",
    "Python implements a standard database API over all databases. Its called [DBAPI2](http://cewing.github.io/training.codefellows/lectures/day21/intro_to_dbapi2.html). It works across many SQL databases.\n",
    "\n",
    "There is an even higher level API available, called [SQLAlchemy](http://www.sqlalchemy.org). Thoroughly recommend it, either in its direct relational form, or ORM form. Many things in Pandas use it to interface with databases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
