{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB: An append-only, disk based, BST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutability in data structures\n",
    "\n",
    "Last time, when we learnt about threads, we saw that mutability could be a big concern: we'd have to lock data structures (or parts thereof), worry about simultaneous reads/writes or writes/writes. We'll see today (and you will implement in your project), an example of how an immutable data structure makes some of these considerations easier. We'll thus also revisit some of the isolation and atomicity issues we talked about in the context of databases.\n",
    "\n",
    "In a world of multi-core computers where lots of events must be handled, we should be thinking of ways to not have to deal with so much state.\n",
    "\n",
    "Functional programming has this notion of **referential transparency** where functions dont modify state: they are like maths functions: we can always substitute the answer in place of the function because some hidden \"instance\" variable encapsulating state is not allowed to change a functions behaviour in functional programming.\n",
    "\n",
    "The key idea to get out of that is immutability of bindings and data structures. \n",
    "\n",
    "We can use this idea in writing databases to similify things and make implementing isolation levels easier with no-overwriting.\n",
    "Consider the example of an immutable linked list, and catenations and updates, things you might think are incompatible with immutability\n",
    "\n",
    "(diagrams from Okasaki's Purely Functional Data Structures)\n",
    "\n",
    "Imperative catenation:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/listcatimp.png)\n",
    "\n",
    "Note that `xs` and `ys` lose their identity as linked lists and are thus destroyed.\n",
    "\n",
    "Now consider doing this immutably:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/listcatfn.png)\n",
    "\n",
    "`xs` and `ys` are unaffected. There is immutability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider updates (notice the sharing: the first two nodes are copied and the last two are shared)\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/listupdatefn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutable BST (unbalanced)\n",
    "\n",
    "For now, lets create an unbalanced BST:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/immutablebst.png)\n",
    "\n",
    "We will look at transactional scenarios regarding atomicity and isolation using this append based binary-tree key-value database or index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An implementation\n",
    "\n",
    "Simplified and taken from http://aosabook.org/en/500L/dbdb-dog-bed-database.html, which you should read. The whole book is good, there is an example of a Transactonal Datomic-like database written in clojure there, as well.\n",
    "\n",
    "In real life this should be a balanced tree, and you will do that in your project. This might help: http://scottlobdell.me/2016/02/purely-functional-red-black-trees-python/ and https://dl.dropboxusercontent.com/u/75194/okasakiredblack99.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueRef(object):\n",
    "    \" a reference to a string value on disk\"\n",
    "    def __init__(self, referent=None, address=0):\n",
    "        self._referent = referent #value to store\n",
    "        self._address = address #address to store at\n",
    "        \n",
    "    @property\n",
    "    def address(self):\n",
    "        return self._address\n",
    "    \n",
    "    def prepare_to_store(self, storage):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def referent_to_bytes(referent):\n",
    "        return referent.encode('utf-8')\n",
    "\n",
    "    @staticmethod\n",
    "    def bytes_to_referent(bytes):\n",
    "        return bytes.decode('utf-8')\n",
    "\n",
    "    \n",
    "    def get(self, storage):\n",
    "        \"read bytes for value from disk\"\n",
    "        if self._referent is None and self._address:\n",
    "            self._referent = self.bytes_to_referent(storage.read(self._address))\n",
    "        return self._referent\n",
    "\n",
    "    def store(self, storage):\n",
    "        \"store bytes for value to disk\"\n",
    "        #called by BinaryNode.store_refs\n",
    "        if self._referent is not None and not self._address:\n",
    "            self.prepare_to_store(storage)\n",
    "            self._address = storage.write(self.referent_to_bytes(self._referent))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "class BinaryNodeRef(ValueRef):\n",
    "    \"reference to a btree node on disk\"\n",
    "    \n",
    "    #calls the BinaryNode's store_refs\n",
    "    def prepare_to_store(self, storage):\n",
    "        \"have a node store its refs\"\n",
    "        if self._referent:\n",
    "            self._referent.store_refs(storage)\n",
    "\n",
    "    @staticmethod\n",
    "    def referent_to_bytes(referent):\n",
    "        \"use pickle to convert node to bytes\"\n",
    "        return pickle.dumps({\n",
    "            'left': referent.left_ref.address,\n",
    "            'key': referent.key,\n",
    "            'value': referent.value_ref.address,\n",
    "            'right': referent.right_ref.address,\n",
    "        })\n",
    "\n",
    "    @staticmethod\n",
    "    def bytes_to_referent(string):\n",
    "        \"unpickle bytes to get a node object\"\n",
    "        d = pickle.loads(string)\n",
    "        return BinaryNode(\n",
    "            BinaryNodeRef(address=d['left']),\n",
    "            d['key'],\n",
    "            ValueRef(address=d['value']),\n",
    "            BinaryNodeRef(address=d['right']),\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryNode(object):\n",
    "    @classmethod\n",
    "    def from_node(cls, node, **kwargs):\n",
    "        \"clone a node with some changes from another one\"\n",
    "        return cls(\n",
    "            left_ref=kwargs.get('left_ref', node.left_ref),\n",
    "            key=kwargs.get('key', node.key),\n",
    "            value_ref=kwargs.get('value_ref', node.value_ref),\n",
    "            right_ref=kwargs.get('right_ref', node.right_ref),\n",
    "        )\n",
    "\n",
    "    def __init__(self, left_ref, key, value_ref, right_ref):\n",
    "        self.left_ref = left_ref\n",
    "        self.key = key\n",
    "        self.value_ref = value_ref\n",
    "        self.right_ref = right_ref\n",
    "\n",
    "    def store_refs(self, storage):\n",
    "        \"method for a node to store all of its stuff\"\n",
    "        self.value_ref.store(storage)\n",
    "        #calls BinaryNodeRef.store. which calls\n",
    "        #BinaryNodeRef.prepate_to_store\n",
    "        #which calls this again and recursively stores\n",
    "        #the whole tree\n",
    "        self.left_ref.store(storage)\n",
    "        self.right_ref.store(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryTree(object):\n",
    "    \"Immutable Binary Tree class. Constructs new tree on changes\"\n",
    "    def __init__(self, storage):\n",
    "        self._storage = storage\n",
    "        self._refresh_tree_ref()\n",
    "\n",
    "    def commit(self):\n",
    "        \"changes are final only when committed\"\n",
    "        #triggers BinaryNodeRef.store\n",
    "        self._tree_ref.store(self._storage)\n",
    "        #make sure address of new tree is stored\n",
    "        self._storage.commit_root_address(self._tree_ref.address)\n",
    "\n",
    "    def _refresh_tree_ref(self):\n",
    "        \"get reference to new tree if it has changed\"\n",
    "        self._tree_ref = BinaryNodeRef(\n",
    "            address=self._storage.get_root_address())\n",
    "\n",
    "    def get(self, key):\n",
    "        \"get value for a key\"\n",
    "        #your code here\n",
    "        #if tree is not locked by another writer\n",
    "        #refresh the references and get new tree if needed\n",
    "        if not self._storage.locked:\n",
    "            self._refresh_tree_ref()\n",
    "        #get the top level node\n",
    "        node = self._follow(self._tree_ref)\n",
    "        #traverse until you find appropriate node\n",
    "        while node is not None:\n",
    "            if key < node.key:\n",
    "                node = self._follow(node.left_ref)\n",
    "            elif key < node.key:\n",
    "                node = self._follow(node.right_ref)\n",
    "            else:\n",
    "                return self._follow(node.value_ref)\n",
    "        raise KeyError\n",
    "\n",
    "    def set(self, key, value):\n",
    "        \"set a new value in the tree. will cause a new tree\"\n",
    "        #try to lock the tree. If we succeed make sure\n",
    "        #we dont lose updates from any other process\n",
    "        if self._storage.lock():\n",
    "            self._refresh_tree_ref()\n",
    "        #get current top-level node and make a value-ref\n",
    "        node = self._follow(self._tree_ref)\n",
    "        value_ref = ValueRef(value)\n",
    "        #insert and get new tree ref\n",
    "        self._tree_ref = self._insert(node, key, value_ref)\n",
    "        \n",
    "    \n",
    "    def _insert(self, node, key, value_ref):\n",
    "        \"insert a new node creating a new path from root\"\n",
    "        #create a tree ifnthere was none so far\n",
    "        if node is None:\n",
    "            new_node = BinaryNode(\n",
    "                BinaryNodeRef(), key, value_ref, BinaryNodeRef())\n",
    "        elif key < node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                left_ref=self._insert(\n",
    "                    self._follow(node.left_ref), key, value_ref))\n",
    "        elif key > node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                right_ref=self._insert(\n",
    "                    self._follow(node.right_ref), key, value_ref))\n",
    "        else: #create a new node to represent this data\n",
    "            new_node = BinaryNode.from_node(node, value_ref=value_ref)\n",
    "        return BinaryNodeRef(referent=new_node)\n",
    "\n",
    "    def delete(self, key):\n",
    "        \"delete node with key, creating new tree and path\"\n",
    "        if self._storage.lock():\n",
    "            self._refresh_tree_ref()\n",
    "        node = self._follow(self._tree_ref)\n",
    "        self._tree_ref = self._delete(node, key)\n",
    "        \n",
    "    def _delete(self, node, key):\n",
    "        \"underlying delete implementation\"\n",
    "        if node is None:\n",
    "            raise KeyError\n",
    "        elif key < node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                left_ref=self._delete(\n",
    "                    self._follow(node.left_ref), key))\n",
    "        elif key > node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                right_ref=self._delete(\n",
    "                    self._follow(node.right_ref), key))\n",
    "        else:\n",
    "            left = self._follow(node.left_ref)\n",
    "            right = self._follow(node.right_ref)\n",
    "            if left and right:\n",
    "                replacement = self._find_max(left)\n",
    "                left_ref = self._delete(\n",
    "                    self._follow(node.left_ref), replacement.key)\n",
    "                new_node = BinaryNode(\n",
    "                    left_ref,\n",
    "                    replacement.key,\n",
    "                    replacement.value_ref,\n",
    "                    node.right_ref,\n",
    "                )\n",
    "            elif left:\n",
    "                return node.left_ref\n",
    "            else:\n",
    "                return node.right_ref\n",
    "        return BinaryNodeRef(referent=new_node)\n",
    "\n",
    "    def _follow(self, ref):\n",
    "        \"get a node from a reference\"\n",
    "        #calls BinaryNodeRef.get\n",
    "        return ref.get(self._storage)\n",
    "    \n",
    "    def _find_max(self, node):\n",
    "        while True:\n",
    "            next_node = self._follow(node.right_ref)\n",
    "            if next_node is None:\n",
    "                return node\n",
    "            node = next_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "\n",
    "import portalocker\n",
    "\n",
    "\n",
    "class Storage(object):\n",
    "    SUPERBLOCK_SIZE = 4096\n",
    "    INTEGER_FORMAT = \"!Q\"\n",
    "    INTEGER_LENGTH = 8\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self._f = f\n",
    "        self.locked = False\n",
    "        #we ensure that we start in a sector boundary\n",
    "        self._ensure_superblock()\n",
    "\n",
    "    def _ensure_superblock(self):\n",
    "        \"guarantee that the next write will start on a sector boundary\"\n",
    "        self.lock()\n",
    "        self._seek_end()\n",
    "        end_address = self._f.tell()\n",
    "        if end_address < self.SUPERBLOCK_SIZE:\n",
    "            self._f.write(b'\\x00' * (self.SUPERBLOCK_SIZE - end_address))\n",
    "        self.unlock()\n",
    "\n",
    "    def lock(self):\n",
    "        \"if not locked, lock the file for writing\"\n",
    "        if not self.locked:\n",
    "            portalocker.lock(self._f, portalocker.LOCK_EX)\n",
    "            self.locked = True\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def unlock(self):\n",
    "        if self.locked:\n",
    "            self._f.flush()\n",
    "            portalocker.unlock(self._f)\n",
    "            self.locked = False\n",
    "\n",
    "    def _seek_end(self):\n",
    "        self._f.seek(0, os.SEEK_END)\n",
    "\n",
    "    def _seek_superblock(self):\n",
    "        \"go to beginning of file which is on sec boundary\"\n",
    "        self._f.seek(0)\n",
    "\n",
    "    def _bytes_to_integer(self, integer_bytes):\n",
    "        return struct.unpack(self.INTEGER_FORMAT, integer_bytes)[0]\n",
    "\n",
    "    def _integer_to_bytes(self, integer):\n",
    "        return struct.pack(self.INTEGER_FORMAT, integer)\n",
    "\n",
    "    def _read_integer(self):\n",
    "        return self._bytes_to_integer(self._f.read(self.INTEGER_LENGTH))\n",
    "\n",
    "    def _write_integer(self, integer):\n",
    "        self.lock()\n",
    "        self._f.write(self._integer_to_bytes(integer))\n",
    "\n",
    "    def write(self, data):\n",
    "        \"write data to disk, returning the adress at which you wrote it\"\n",
    "        #first lock, get to end, get address to return, write size\n",
    "        #write data, unlock <==WRONG, dont want to unlock here\n",
    "        #your code here\n",
    "        self.lock()\n",
    "        self._seek_end()\n",
    "        object_address = self._f.tell()\n",
    "        self._write_integer(len(data))\n",
    "        self._f.write(data)\n",
    "        return object_address\n",
    "\n",
    "    def read(self, address):\n",
    "        self._f.seek(address)\n",
    "        length = self._read_integer()\n",
    "        data = self._f.read(length)\n",
    "        return data\n",
    "\n",
    "    def commit_root_address(self, root_address):\n",
    "        self.lock()\n",
    "        self._f.flush()\n",
    "        #make sure you write root address at position 0\n",
    "        self._seek_superblock()\n",
    "        #write is atomic because we store the address on a sector boundary.\n",
    "        self._write_integer(root_address)\n",
    "        self._f.flush()\n",
    "        self.unlock()\n",
    "\n",
    "    def get_root_address(self):\n",
    "        #read the first integer in the file\n",
    "        #your code here\n",
    "        self._seek_superblock()\n",
    "        root_address = self._read_integer()\n",
    "        return root_address\n",
    "\n",
    "    def close(self):\n",
    "        self.unlock()\n",
    "        self._f.close()\n",
    "\n",
    "    @property\n",
    "    def closed(self):\n",
    "        return self._f.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBDB(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self._storage = Storage(f)\n",
    "        self._tree = BinaryTree(self._storage)\n",
    "\n",
    "    def _assert_not_closed(self):\n",
    "        if self._storage.closed:\n",
    "            raise ValueError('Database closed.')\n",
    "\n",
    "    def close(self):\n",
    "        self._storage.close()\n",
    "\n",
    "    def commit(self):\n",
    "        self._assert_not_closed()\n",
    "        self._tree.commit()\n",
    "\n",
    "    def get(self, key):\n",
    "        self._assert_not_closed()\n",
    "        return self._tree.get(key)\n",
    "\n",
    "    def set(self, key, value):\n",
    "        self._assert_not_closed()\n",
    "        return self._tree.set(key, value)\n",
    "\n",
    "    def delete(self, key):\n",
    "        self._assert_not_closed()\n",
    "        return self._tree.delete(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def connect(dbname):\n",
    "    try:\n",
    "        f = open(dbname, 'r+b')\n",
    "    except IOError:\n",
    "        fd = os.open(dbname, os.O_RDWR | os.O_CREAT)\n",
    "        f = os.fdopen(fd, 'r+b')\n",
    "    return DBDB(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm /tmp/test2.dbdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "db = connect(\"/tmp/test2.dbdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.set(\"rahul\", \"aged\")\n",
    "db.set(\"pavlos\", \"aged\")\n",
    "db.set(\"kobe\", \"stillyoung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-31905ff9ed7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/test2.dbdb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rahul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-d0e8d3b6ca15>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-048548199505>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_follow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.set(\"rahul\", \"aged\")\n",
    "db.set(\"pavlos\", \"aged\")\n",
    "db.set(\"kobe\", \"stillyoung\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.close()\n",
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'young'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.set(\"rahul\", \"young\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.close()\n",
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'young'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.set(\"rahul\", \"young\")\n",
    "db.commit()\n",
    "db.close()\n",
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.delete(\"pavlos\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critical parts of the code from above are these:\n",
    "\n",
    "from Storage:\n",
    "\n",
    "```python\n",
    "    def commit_root_address(self, root_address):\n",
    "        self.lock()\n",
    "        self._f.flush()\n",
    "        #make sure you write root address at position 0\n",
    "        self._seek_superblock()\n",
    "        #write is atomic because we store the address on a sector boundary.\n",
    "        self._write_integer(root_address)\n",
    "        self._f.flush()\n",
    "        self.unlock()\n",
    "        \n",
    "    def _ensure_superblock(self):\n",
    "        \"guarantee that the next write will start on a sector boundary\"\n",
    "        self.lock()\n",
    "        self._seek_end()\n",
    "        end_address = self._f.tell()\n",
    "        if end_address < self.SUPERBLOCK_SIZE:\n",
    "            self._f.write(b'\\x00' * (self.SUPERBLOCK_SIZE - end_address))\n",
    "        self.unlock()\n",
    "```\n",
    "\n",
    "get and set on the BinaryTree:\n",
    "\n",
    "```python\n",
    "    def get(self, key):\n",
    "        \"get value for a key\"\n",
    "        #your code here\n",
    "        #if tree is not locked by another writer\n",
    "        #refresh the references and get new tree if needed\n",
    "        if not self._storage.locked:\n",
    "            self._refresh_tree_ref()\n",
    "        #get the top level node\n",
    "        node = self._follow(self._tree_ref)\n",
    "        #traverse until you find appropriate node\n",
    "        while node is not None:\n",
    "            if key < node.key:\n",
    "                node = self._follow(node.left_ref)\n",
    "            elif key < node.key:\n",
    "                node = self._follow(node.right_ref)\n",
    "            else:\n",
    "                return self._follow(node.value_ref)\n",
    "        raise KeyError\n",
    "\n",
    "    def set(self, key, value):\n",
    "        \"set a new value in the tree. will cause a new tree\"\n",
    "        #try to lock the tree. If we succeed make sure\n",
    "        #we dont lose updates from any other process\n",
    "        if self._storage.lock():\n",
    "            self._refresh_tree_ref()\n",
    "        #get current top-level node and make a value-ref\n",
    "        node = self._follow(self._tree_ref)\n",
    "        value_ref = ValueRef(value)\n",
    "        #insert and get new tree ref\n",
    "        self._tree_ref = self._insert(node, key, value_ref)\n",
    "        \n",
    "    def _write_integer(self, integer):\n",
    "        self.lock()\n",
    "        self._f.write(self._integer_to_bytes(integer))\n",
    "\n",
    "    def write(self, data):\n",
    "        \"write data to disk, returning the adress at which you wrote it\"\n",
    "        #first lock, get to end, get address to return, write size\n",
    "        #write data, unlock\n",
    "        #your code here\n",
    "        self.lock()\n",
    "        self._seek_end()\n",
    "        object_address = self._f.tell()\n",
    "        self._write_integer(len(data))\n",
    "        self._f.write(data)\n",
    "        return object_address\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out the address of the root node is atomic since we are on a disk sector boundary, and this is the very first thing in the file. The disk-hardware guarantees that single-sector writes are atomic. \n",
    "\n",
    "The above case is atomic in the single-object sense because the changes are made and comitted, creating a new version of the tree. If things fail, the tree wont be created and you are back to the previous version. We havent explicitly built in any abort-handling though, relying on the python exceptions to do this for us. Another process will NEVER see a mix of writes: the root address will either be the old value or the new one. \n",
    "\n",
    "Our very own in memory process, will, however, reflect the partial changes we made, so a proper transaction manager should kill the \"thread\" of execution. But we havent talked about a process model yet, and will come to this next week. Today we'll assume we have different processes (so that we dont have to worry about the GIL in thinking about any of this).\n",
    "\n",
    "We also write the new data (the new tree) to disk (and flush) before we update the root address, and thus there is no way to get at them as yet. When the root address is updated, we are guaranteed its data is now on disk. This ensures durability: once the transaction is comitted, its data are on disk, and until it is comitted, this data is not reachable, indeed only the old data is (even if the new data is eating up pprecious bytes).\n",
    "\n",
    "But what about isolation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is isolation important?\n",
    "\n",
    "It is hard to program without isolation. isolation is what guarantees stability. It is what makes sure that there are no dirty reads and dirty writes.\n",
    "\n",
    "From designing data intensive applications:\n",
    "\n",
    "Dirty reads\n",
    "\n",
    ">One client reads another client’s writes before they have been committed. The read committed isolation level and stronger levels prevent dirty reads.\n",
    "\n",
    "\n",
    "Dirty writes\n",
    "\n",
    ">One client overwrites data that another client has written, but not yet committed. Almost all transaction implementations prevent dirty writes.\n",
    "\n",
    "Clearly, the notions of isolation are really the notions of concurrenvy: these issues will also occur when 2 programs access any data, in memory or in a database. In both cases locks and other ideas must be used to make sure that there is only one mutator at a time, and that an object is not exposed in an inconsistent state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Committed\n",
    "\n",
    "No dirty reads and no dirty writes.\n",
    "\n",
    "Dirty reads means that you can peek into the goings on inside a transaction and get at itermediate values. This could mean you see a value that would be later rolled back.\n",
    "\n",
    "with dirty writes you could be overwriting an uncomitted value\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/dirtywrite.png)\n",
    "\n",
    "In Read Comitted mode, in essence we need to remember the pre-transaction value, ie the old comitted value and the also the new value being set by the transaction which holds the current write lock. While this transaction runs, any other thread/transaction sees the old value. Once this thread commits, then the values read by the other transaction are switched. And once the lock is held, no-one else can write.\n",
    "\n",
    "You can see this in the `get` above where we check whether any other transaction is running and if not get the latest tree. In a regular database we similarly use row-level locks. Another writer must wait until the lock is given up, and the reader will read the value before the lock was set as long as the write transaction is in progress.\n",
    "\n",
    "So if we implemented a transaction system for our database, the current code would put it in Read Committed mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Read Skew Problem\n",
    "\n",
    ">Non-repeatable reads:\n",
    ">A client sees different parts of the database at different points in time. This is most commonly prevented with snapshot isolation, which allows a transaction to read from a consistent snapshot at one point in time. It is usually implemented with multi-version concurrency control (MVCC).\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/readskew.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the diagram above, it makes perfect sence if Alice's two reads are separate read transactions. But what if they are one transaction and we are in read-comitted mode. Since the reads took a very long time and were on both sides of a kosher transaction, they might give Alice a heart attack. This is a non-repeatable read as if Alice were to re-issue the reas transaction, acct 1 would now show 600. This problem is perfectly consistent with operations in read-comitted mode: the second read gets the switched value in analog to our get.\n",
    "\n",
    "The fix for this would seem to be clear: if both the reads happen in one transaction, one must make sure that because this transaction happened before the write, it sees the older version of the accounts (500, 500). Cast into the language of our database, the check we do for a get ought to be suspended if the get is inside an already started transaction. In general, the only place a tree-refresh ought to be allowed is at the beginning of a read-or write transaction. So we would need to implement a `TransactionManager` whose job it would be to keep track of this.\n",
    "\n",
    "These thoughts form the basis of Multi-Version concurrency control, or MVCC, also known as snapshot isolation.\n",
    "\n",
    "The key principle is (from DDIA):\n",
    "> readers never block writers, and writers never block readers\n",
    "\n",
    "Here then are the rules:\n",
    "\n",
    ">1. At the start of each transaction, the database makes a list of all the other transac‐ tions which are in progress (not yet committed or aborted) at that time. Any writes made by one of those transactions are ignored, even if the transaction sub‐ sequently commits.\n",
    "2. Any writes made by aborted transactions are ignored.\n",
    "3. Any writes made by transactions with a later transaction ID (i.e. which started after the current transaction started) are ignored, regardless of whether that transaction has committed.\n",
    "4. All other writes are visible to the application’s queries.\n",
    "\n",
    "Here's what snapshot isolation using MVCC looks like:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/mvcc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about writes?\n",
    "\n",
    "There is another problem we encounter here: \n",
    "\n",
    "### the lost update problem.\n",
    "\n",
    "Consider an upsert, where we read a value, do something with it, like the balance change above, and then write it. Because we only lock writes, a transaction T1 might read a value V0, and then transaction T2 reads value V0. Since T2 does not have lock, the code in `get` wont get a new tree, and uses the old value. This is good as the transaction T1 is still running.\n",
    "\n",
    "But here is the problem: T1 will create a new tree, say with V0+1. Now T2 comes along, operates on V0, and makes it V0+2. say V0 was 5 to start with. The notion of the process might have been 5->6->8. But we wont do that. Our latest value will be 7.\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/lostupdate.png)\n",
    "\n",
    "The fix to this is atomic updates/upserts. The `UPDATE` command in sql does just that, and if we build transactional facility into our database to do multiple gets, there is no reason not to extend it to an upsert. The entire upsert would grab the lock, and once that happens this would effectively serialize T1 and T2. Atomic updates are usually thus built by taking the lock when the object is read, or by forcing all atomic operations to be on one given thread.\n",
    "\n",
    "So thus it seems we can add a transaction manager, have an explicit notion of read, write, and upsert transactions, and have a reasonable snapshot isolated database.\n",
    "\n",
    "But even this does not solve some problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your project\n",
    "\n",
    "I'll hand out a more formal spec for this Monday, but informally speaking, the next part of your project is to use the resources linked to, in this lab, to construct a functional red-black tree, which serializes to disk. There will be much more writing to disk due to updates. You do not need to implement deletion (deletion is complex!). A link above (Scott Dobell's blog) does show you how to make in-memory red black trees but you should read the paper to understand how they work.\n",
    "\n",
    "Your database should work as a python library, and should be usable in a multithreaded-multiprocess socket-fronted solution. You will learn how to do this next wednesday. For your current work, the idea is simply to have one process read and write stuff with commits, exactly as shown in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World usage\n",
    "\n",
    "These functional ideas are in use (for btrees) in the real world databases lmdb (used in openldap) and boltdb (a very popular database written in Go)\n",
    "\n",
    "There, we move from append-Only, which works best with a good garbage collector:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/append1.png)\n",
    "\n",
    "to copy on write\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/cow1.png)\n",
    "![](https://dl.dropboxusercontent.com/u/75194/cow2.png)\n",
    "![](https://dl.dropboxusercontent.com/u/75194/cow3.png)\n",
    "\n",
    "See https://dl.dropboxusercontent.com/u/75194/20141120-BuildStuff-Lightning.pdf\n",
    "\n",
    "which is functional with a memory of 1-2 versions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
